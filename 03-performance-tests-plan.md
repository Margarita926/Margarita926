# Модуль 3 · Практичне завдання 2
## План тестування продуктивності

## 1. Executive Summary
- Загальний підхід до тестування
- Ключові цілі та пріоритети
- Зв'язок з результатами Модуля 2

## 2. Детальний план тестів
1) Типи тестів та приклади сценаріїв
- Load Test: типові user journeys, ramp-up до 100 VUs, sustain 2m, target ~100 RPS per endpoint
- Stress Test: збільшення до 150–200% від піку, утримання 10m, моніторинг recovery
- Soak Test: 24h при 200 VUs (перевірка memory leaks)
- Spike Test: instant spike 10× baseline на 5m, відновлення 10m
- Breakpoint Test: крокове збільшення VUs до відмови (100→200→400→800)
- Configuration Test: порівняння вертикального vs горизонтального scaling

2) Параметри (приклад для k6)
- Load: stages: 30s→10 VUs, 2m→50 VUs, 1m→100 VUs, sustain as required
- Timeouts: http timeout 5s (збільшити при потребі)
- Think time: random 1–3s

3) Критерії успішності (приклад)
- p95 < 500 ms — PASS
- Error rate < 1% — PASS
- Throughput ≥ target RPS — PASS

4) Метрики для збору
- Application: latency percentiles, error counts
- System: CPU, memory, disk I/O, network
- Infrastructure: DB connections, queue depth, GC

5) Команди та пріоритети
- Першочергові тести: Smoke → Load → Stress. Soak та Breakpoint — в окремому вікні через довгу тривалість.

### 2.1 Load Testing
| Параметр | Значення | Обґрунтування |
|----------|----------|---------------|
| VUs      | 200–500  | Відповідає середньому очікуваному навантаженню сервісу у пікові години |
| Duration | 30'      | Достатньо для збору метрик |
| Success Criteria| 95% запитів ≤ 2 сек; Error rate ≤ 1% | швидкість доставки, низький рівень помилок |

### 2.2 Stress Testing
визначити межу продуктивності
1. Стабільне навантаження на відомій потужності, встановлення базових метрик(5-10 хв)
2. Поступове збільшення (30-45 хв) +25% кожні 5-10 хв, моніторинг деградації
3. Точка розриву (10-15 хв)Продовження до чіткої відмови, визначення режиму відмови
4. Валідація відновлення (10-15 хв) Зменшення до базових рівнів, перевірка відновлення
   
- Вимірювання часу відновлення після зниження навантаження
- Фіксація точок відмови та аналіз логів для виявлення  помилок

### 2.3 Soak Testing
перевірити стабільність роботи системи протягом тривалого часу
1. Постійне навантаження у 300 Virtual users протягом 12-24 годин
2. Тривале навантаження у 500 Virtual users протягом 48 годин для виявлення memory leaks

- Моніторінг використання пам’яті для виявлення memory leaks недостачі
- Вимірювання деградації продуктивності (зростання часу відповіді, накопичення помилок)
- Перевірка стабільності бд, кешу та мережевих з’єднань


### 2.4 Spike Testing
оцінити реакцію системи на різкі піки навантаження
1. Збільшення кількості користувачів 100 → 1000 за 1 хвилину
2. Масовий одночасний виклик веб‑хуків із різних інтеграцій
3. Імітація несподіваного піку активності під час бізнес‑критичної операції (наприклад логін чи платіж)
   
- Перевірка  auto-scaling роботи
- Аналіз швидкості адаптації системи, часу стабілізації, помилок
- Виявлення помилок при несподіваних піках активності
  
### 2.5 Breakpoint Testing
 визначити максимальну пропускну здатність системи
1. Поступове збільшення навантаження з 200 → 2000 vUsers до моменту відмови

- Фіксація критичних точок (бд перестає відповідати чи втрачається Latency, сервери падають) 
- Аналіз cascade failures.Визначення порогу, після якого система більше не може обробляти запити
   

### 2.6 Configuration Testing
оптимізувати налаштування системи для максимальної продуктивності
1. Тестування різних розмірів пулу з’єднань 10, 50, 100
2. Порівняння конфігурацій кешування (без кешу, Redis, in‑memory)

Вибір оптимальних налаштувань для досягнення SLO

## 3. Chaos Engineering

### 3.1 Infrastructure Scenarios
1. Вимкнення одного з серверів, перевірка роботи механізмів відмов
2. Імітація втрати мережевого з’єднання між сервісами
3. Відключення одного з дата‑центрів, перевірка гео‑ та балансування
4. Перевірка поведінки при частковій недоступності load balancer

### 3.2 Service Degradation
1. Штучне уповільнення зовнішніх API
2. Імітація втрати пакетів у мережі (10–20%)
3. Введення штучної затримки у виклики сторонніх сервісів
4. Перевірка поведінки при часткових помилках (30% запитів повертають 500).

### 3.3 Resource Constraints
1. Обмеження CPU до 50% від доступного
2. Штучне зменшення доступної пам’яті (наприклад до 256 MB)
3. Заповнення дискового капасіті до критичного рівня
4. Перевірка поведінки при обмеженні кількості відкритих сокетів/з’єднань

## 4. Обґрунтування та пріоритизація

### 4.1 Бізнес-цінність
- Load Testing: перевіряє, що основні бізнес-функції (пошук, оформлення замовлення, автентифікація) працюють при очікуваному навантаженні — зниження ризику втрати доходу і погіршення UX.
- Stress Testing: виявляє межі системи — критично для планування capacity та уникнення простоїв під час маркетингових кампаній.
- Soak Testing: виявляє проблеми стабільності (memory leaks, деградацію) — важливо для довготривалої доступності сервісу.
- Spike Testing: перевіряє еластичність і швидкість auto-scaling — мінімізує ризик значних імпактів під час раптових піків трафіку.
- Breakpoint / Chaos: визначає точки відмови та план відновлення — знижує ризик cascade-failures і втрати даних.

### 4.2 Технічні ризики
- Database connection pool exhaustion — призводить до timeouts і підвищеного error rate.
- Message queue backlog (RabbitMQ) — збільшення latency фонового оброблення.
- Неправильні таймаути або короткі таймаути на клієнті — штучні помилки при підвищеному навантаженні.
- GC pauses / memory leaks у сервісах — поступова деградація продуктивності під час Soak.
- Зовнішні API (third-party) — непередбачувані затримки або помилки, які впливають на загальну пропускну здатність.
- Мережеві обмеження або firewall — втручання в пропускну здатність та доступність.


## 4.3 Матриця пріоритетів
| Тест | Пріоритет | Impact | Effort | Обґрунтування |
|------|-----------|--------:|-------:|---------------|
| Load Testing | High | High | Medium | Ключовий для підтвердження SLO та UX під середнім/піковим навантаженням |
| Stress Testing | High | High | High | Визначає межі системи і recovery behavior — критично для capacity planning |
| Spike Testing | High | High | Medium | Реалістичні сценарії маркетингових піків; перевірка авто-скейлу |
| Soak Testing | Medium | Medium | High | Виявлення memory leaks; важливий для довготривалої стабільності, але довгий за часом |
| Breakpoint Testing | Medium | Medium | High | Служить для пошуку крайньої точки відмови; ресурсозатратно |
| Configuration Testing | Low | Low | Medium | Оптимізація конфігурацій додає ефективності, але не критична на першому етапі |

---

## 5. Метрики та моніторинг
- Ключові індикатори продуктивності (KPI): p50 / p95 / p99 latency, average latency, throughput (RPS), error rate (%), availability (%), request/transaction per second, queue depth, DB connection count, GC pause times.
- Додаткові системні метрики: CPU %, Memory MB, Disk I/O, Network I/O, thread pool saturation.
- Інструменти збору метрик: k6 (load metrics), Prometheus (metrics collection), Grafana (dashboards), ELK/Tempo (логування, трасування), RabbitMQ Management, database monitoring (pg_stat / DMVs), WireMock logs для зовнішніх викликів.
- Alerting та escalation procedures:
	- Визначити thresholds (приклад): p95 > 500ms → Warning, p95 > 1000ms → Critical; error rate > 1% → Warning, >5% → Critical; availability drop > SLA → Critical.
	- Нотифікація: Slack / PagerDuty / Email на on-call інженера.
	- Ескалація: Severity levels (P1/P2/P3) — P1 (SLA breach / production outage) → immediate page + incident bridge; P2 (degradation) → notify dev leads; P3 (minor) → ticket backlog.
	- Runbooks: базові кроки для діагностики (health checks, restart instances, check DB connections, inspect logs), contact list та escalation times.

## 6. Висновки
- Готовність до виконання тестів: середовище має базові вимоги; перед запуском великих сценаріїв — перевірити health endpoints, логи та мережеву доступність.
- Ризики та mitigation strategies:
	- Якщо DB pool exhaust — збільшити pool або масштабувати DB read replicas; оптимізувати запити.
	- Для черг — моніторити backlog і скоригувати consumer parallelism.
	- Для third-party залежностей — використовувати WireMock для стабільних відповідей під час тестів або впровадити захист (circuit breaker).
- Next steps для Модуля 4:
	1. Описати інфраструктуру з точними ресурсами (flavors, CPU/RAM) і створити `docker-compose` або k8s manifests для тестового середовища.
	2. Налаштувати Prometheus + Grafana та зберегти dashboard templates.
	3. Підготувати runbooks та alert rules; виконати smoke test перед великим запуском.

---
